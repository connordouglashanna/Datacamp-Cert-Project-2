---
title: "Datacamp Cert Project w/ Text"
author: "Connor Hanna"
date: '2022-11-10'
output: pdf_document
---

# Data Scientist Associate Case Study

## Company Background
EMO is a manufacturer of motorcycles. The company successfully launched its first electric moped in India in 2019. The product team knows how valuable owner reviews are in making improvements to their mopeds. 

Unfortunately they often get reviews from people who never owned the moped. They don’t want to consider this feedback, so would like to find a way to identify reviews from these people. They have obtained data from other mopeds, where they know if the reviewer owned the moped or not. They think this is equivalent to their own reviews.   


## Customer Question
Your manager has asked you to answer the following:
- Can you predict which reviews come from people who have never owned the moped before?  

## Dataset
The dataset contains reviews about other mopeds from a local website. The data you will use for this analysis can be accessed here: `"data/moped.csv"`

| Column Name      | Criteria                                                                                                                                                                    |
|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Used it for      | Character, the purpose of the electric moped for the user, one of “Commuting”, “Leisure”.                                                                                   |
| Owned for        | Character, duration of ownership of vehicle one of  “<= 6 months”, “> 6 months”, “Never Owned”.  Rows that indicate ownership should be combined into the category “Owned”. |
| Model name       | Character, the name of the electric moped.                                                                                                                                   |
| Visual Appeal    | Numeric, visual appeal rating (on a 5 point scale, replace missing values with 0).                                                                                           |
| Reliability      | Numeric, reliability rating (on a 5 point scale, replace missing values with 0).                                                                                             |
| Extra Feature    | Numeric, extra feature rating (on a 5 point scale, replace missing values with 0).                                                                                         |
| Comfort          | Numeric, comfort rating (on a 5 point scale, replace missing values with 0).                                                                                                 |
| Maintenance cost | Numeric, maintenance cost rating (on a 5 point scale, replace missing values with 0).                                                                                        |
| Value for money  | Numeric, value for money rating (on a 5 point scale, replace missing values with 0).                                                                                         |

# Data Scientist Associate Case Study Submission

Use this template to complete your analysis and write up your summary for submission.


```{r setup, echo = FALSE}
# installing packages
#install.packages(c("ggthemes", 
#                   "caret",
#                   "xgboost",
#                   "ggforce", 
#                   "vtreat",
#                   "WVPlots"))


# Loading packages
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(colorspace)
library(caret)
library(broom)
library(ggrepel)
library(xgboost)
library(rsample)
library(forcats)
library(ggforce)
library(vtreat)
library(WVPlots)
library(pROC)

# set wd
setwd("~/School/datacamp/Datacamp-Cert-Project-2")

# Importing data from csv
moped <- read_csv("moped.csv")
```

```{r}
# Inspecting data for variables to ensure we're not missing anything
head(moped)

# all present and accounted for
# varnames all have quotation marks though, those are hideous

# renaming variables
moped <- 
moped %>% 
  mutate(used_for = `Used it for`,
         duration_owned = `Owned for`,
         model = `Model Name`,
         visual_appeal = `Visual Appeal`,
         extra_features = `Extra Features`,
         maint_cost = `Maintenance cost`,
         value = `Value for Money`,
         reliability = Reliability,
         comfort = Comfort,
         .keep = "unused") 

# corrected some capital letters too. naming is now consistent. 

# now to correct issues mentioned in the documentation
# "owned for" ownership column should be changed to indicate ownership as a dummy

moped <- 
  moped %>%
  mutate(owned = ifelse(duration_owned == "Never owned", 0, 1), .keep = "unused")

# storing the original moped df for when I want the NAs for vtreat
moped_og <- moped

# checking other variables for NA values
# storing for use later validating manipulation prior to analysis
colSums(is.na(moped))

# replacing NA values with 0 

moped[is.na(moped)] <- 0

colSums(is.na(moped))

# checking for entries outside expected values

summary(moped)
```

## Data Validation
Describe the validation tasks you completed and what you found. Have you made any changes to the data to enable further analysis? Remember to describe what you did for every column in the data. 

I begin the validation by calling `head()` to inspect the variables in the dataframe and make sure I'm not missing anything described in the documentation, and to confirm that the classes of the variables is as described. In some cases, the names were vague or otherwise inappropriate. To correct this, I called `mutate()` to rename the variables for convenience and clarity - in particular `"Owned for"`, where the original variable name was vague with respect to the contents of the variable. I specified the `.keep` argument to remove the original variables.

Then as specified in the documentation I used `mutate()` again to transform `duration_owned` into a binary variable reflecting ownership status. Once again I used `.keep` to remove the original variable. I saved this version of the dataframe for use later, since `vtreat` can transform `NA` values into useful binary indicators and synthesize approximate true values. 

Then, since the documentation described `NA` values in multiple numerical variables, I checked the dataframe for them. After confirming they were only in the variables described, I used `is.na()` to replace them with 0 as instructed, and rechecked to make sure I hadn't left any stragglers. 

I then checked for values outside the expected ranges of the numerical variables, found that there were none, and moved on to the exploratory analysis. 

```{r}
# Exploratory Analysis
# Explore the characteristics of the variables in the data

# One density plot
moped |> 
  ggplot(aes(
    comfort, fill = "#440154FF"
  )) + 
  geom_density() + 
  labs(
    title = "Density plot of comfort ratings in moped reviews",
    x = "Comfort",
    y = "Density"
  ) + 
  theme_minimal() + 
  scale_fill_viridis_d() +
  theme(legend.position = "none")

# density bars of all numerical variables, sorted by ownership
  # pivot longer
  moped %>%
    pivot_longer(visual_appeal:comfort) %>%
    select(value, name, owned) %>%
  # recoding ownership values 
    mutate(owned = ifelse(owned == 0, "Not owned", "Owned")) %>%
  # recoding names of variables for facet titles
    mutate(name = recode(name,
                         "comfort" = "Comfort",
                         "extra_features" = "Extra features",
                         "maint_cost" = "Maintenance cost",
                         "reliability" = "Reliability", 
                         "value" = "Value", 
                         "visual_appeal" = "Visual appeal")) %>%
  # only NA values encoded to 0, we'll leave those out
    filter(value > 0) %>%
  # plot generation
  ggplot(aes(
    value, fill = as.factor(owned), alpha = 0.9
  )) + 
    geom_histogram(breaks = seq(0.5, 5.5, 1), position = "identity", aes(y = ..density..)) + 
    facet_wrap(vars(name), scales = "free_y") + 
    labs(
      title = "Density histogram of ratings, faceted by category, colored by ownership",
      x = "Rating",
      y = "Density", 
      fill = "Ownership"
    ) + 
    guides(fill = "legend", alpha = "none") + 
    scale_fill_manual(values = c(
      'Not owned' = '#EE6A50',
      'Owned' = '#87CEFA'
    ))
      
# proportion of ownership by use 
moped %>%
  mutate(commuter = ifelse(used_for == "Commuting", 1, 0), .keep = "unused") |>
  group_by(commuter) %>%
  summarize(prop_owned = mean(owned), n = n()) %>%
  arrange(prop_owned)

# proportion of ownership by model
moped %>% 
  group_by(model) %>%
  summarize(prop_owned = mean(owned), n = n()) %>%
  arrange(prop_owned)

# pie plot to investigate ownership rate by model
  # generating a list of desired models
  model_list <- 
    moped |>
      group_by(model) |>
      summarize(n = n()) |>
      arrange(desc(n)) |>
      head(n = 20) |>
      pull(var = model) 

  # generating a table with clear aesthetic assignments for ggplot
  moped |>
    # culling low-n models using the list from earlier
    filter(model %in% model_list) |>
    # modifying owned to a factor
    mutate(owned = ifelse(owned == 0, "Not owned", "Owned")) |>
    group_by(model, owned) |>
    summarize(n = n()) |>
  # generating pie chart 
  ggplot(
    aes(
        x0 = 0, y0 = 0,
        r0 = 0, r = 1,
        amount = n,
        fill = owned,
      )
    ) + 
    geom_arc_bar(stat = "pie") + 
    theme_void() + 
    coord_fixed() + 
    labs(title = "Ownership proportion in moped reviews, by model", fill = "Reviewer ownership") + 
    facet_wrap(vars(model)) + 
    scale_fill_manual(values = c(
      'Not owned' = '#EE6A50',
      'Owned' = '#87CEFA'
    )) + 
    theme(
      panel.spacing = unit(0.5, "cm"),
      strip.text = element_text(size = 7)
    )
  
# PCA
  # arrow style object
  arrow_style <- arrow(
    angle = 20, length = grid::unit(8, "pt"),
    ends = "first", type = "closed")
  
  # generating PCA object
  pca_fit <- 
    moped %>%
    # recoding the use variable to a dummy
    mutate(commuter = ifelse(used_for == "Commuting", 1, 0)) %>% 
    # removing legacy/categorical variables
    select(-c(model, used_for)) %>%
    na.omit() %>%
    scale() %>%
    prcomp()
  pca_fit
  
  # rotation matrix
  pca_fit |>
    tidy(matrix = "rotation") |>
    pivot_wider(
      names_from = "PC", values_from = "value",
      names_prefix = "PC"
    ) |>
    # biplot
    ggplot(aes(PC1, PC2)) +
    geom_segment(
      xend = 0, yend = 0,
      arrow = arrow_style
    ) +
    geom_text_repel(aes(label = column)) +
    xlim(-0.75, 0.75) + ylim(-1, 0.5) + 
    coord_fixed()
  
  # fetching the r-squared values for the principle components via eigenvalue plot
  pca_fit |>
    tidy(matrix = "eigenvalues") |>
    # scree plot
    ggplot(aes(PC, percent, fill = PC)) + 
    geom_col() + 
    scale_x_continuous(
      breaks = 1:8
    ) + 
    scale_y_continuous(
      name = "Variance Explained",
      label = scales::label_percent(accuracy = 1)
    ) + 
    scale_fill_viridis_c() +
    theme(legend.position = "none")
  
# bar graph of counts by model name
  moped |>
    ggplot(aes(
      fct_infreq(model), fill = after_stat(count)
    )) + 
    geom_bar() + 
    coord_flip() + 
    labs(
      title = "Total number of reviews for each moped model",
      x = "Model", 
      y = "Count"
    ) + 
    # including line showing the minimum count for inclusion as a group in splitting
    geom_hline(yintercept = 713 * .10, color = "#440154FF") + 
    theme_bw() + 
    scale_fill_viridis_c() + 
    theme(legend.position = "none") 
  
# counts by brand
  moped |> 
    separate(model, into = c("make", "model"), sep = "\\s", extra = "merge") |>
    ggplot(aes(
      fct_infreq(make), fill = after_stat(count)
    )) + 
    geom_bar() + 
    coord_flip() + 
    labs(
      title = "Total number of reviews for each moped manufacturer",
      x = "Make", 
      y = "Count"
    ) + 
    geom_hline(yintercept = 713 * .10, color = "#440154FF") + 
    theme_bw() + 
    scale_fill_viridis_c() + 
    theme(legend.position = "none") 
  
# bar graph of observations in makes vs models meeting the requirements
  # generating the desired summary stats
  model_n_1 <- 
    moped |>
    group_by(model) |>
    mutate(n = n()) |>
    filter(n > 71.3) |>
    count() |>
    mutate(make = NA)
  
  model_n_2 <- 
    moped |> 
    separate(model, into = c("make", "model"), sep = "\\s", extra = "merge") |>
    group_by(make) |>
    mutate(n = n()) |>
    filter(n > 71.3) |>
    count() |>
    mutate(model = NA)
  
  # forming final matrix of information
  model_n <- 
    rbind(model_n_1, model_n_2) |>
    mutate(type = ifelse(is.na(model) == TRUE, 1, 0),
           model = ifelse(type == 1, make, model)) |>
    select(-make) 
  # generating the bar graph 
  model_n |>
    ggplot(aes(
      as.factor(type), n, fill = model
    )) + 
    geom_col() + 
    labs(
      x = "Grouping",
      y = "Count", 
      title = "Captured observations by grouping type",
      fill = "Make/Model name"
    ) +
    scale_x_discrete(
      labels = c("Make", "Model")
    )
  
# examining balance of owned/not owned observations in the data
moped |> 
  group_by(owned) |> 
  summarize(n = n())
```

## Exploratory Analysis
Describe what you found in the exploratory analysis. In your description you should:
- Reference at least two different data visualizations you created above to demonstrate the characteristics of variables
- Reference at least one data visualization you created above to demonstrate the relationship between two or more variables
- Describe what your exploratory analysis has told you about the data
- Describe any changes you have made to the data to enable modeling

*characteristics of a single variable*

I began the data exploration by modeling a single variable using `geom_density()`, `comfort`. The presence of large numbers of `NA` values as seen in the validation appeared here as a large number of zeroes in the density plot. No modification to the data was needed. 

As you can see in the plot, the values reported by reviewers are not normally distributed.

*characteristics of multiple variables*

I then expanded this analysis, transitioning to a bar plot since the variables are all integers. I first used `pivot_longer()` and `select()` to generate a dataframe that contained only the values for the numerical variables accompanied by the variable name. I then transformed the `owned` variable into a character string for easier aesthetic assignments in the plot, recoded the names to be more appropriate for legend generation, removed the recoded `NA` values, and piped the resulting dataframe into `ggplot()`. Attached to the `ggplot()` call I specified the `identity` position and the `..density..` aesthetic in the `geom_histogram()` function to visualize density, faceted by the variable name with a free vertical axis, manually specified the color scheme to color by ownership, and specified labels as I do throughout. Since the values are all integers I specified breaks to bin the integer values neatly. 

The resulting plot shows that non-owner reviewers were more likely to assign higher values to essentially every category, but that no category in particular appeared to be alone capable of identifying a non-owner review. Again none of the distributions of ratings were normal. 

I then generated some summary tables using `group_by()` and `summarize()` to explore a potential relationship between ownership and any of the categorical variables. Commuter mopeds were far less likely to be non-owner reviews, and due to the smaller sample size of many model categories some models had 100% or 0% ownership rates. This warrants consideration when expanding these variables into dummy indicators, since any model incorporating those smaller sample models may not have much external validity when applied to new reviews of those small-sample models. 

*characteristics of multiple variables*

To examine this further, I decided to generate a pie chart of the models with the highest number of observations to determine how many could be included in the final analysis. I started by generating `model_list`, a list of the models with the highest number of observations, by grouping on `model`, summarizing on `n()`, calling `head()` with `n = 20`, and pulling the `model` vector from the resulting dataframe. I then generated an appropriate dataframe for aesthetic assignments by filtering for models in `model_list`, converting `owned` back to a character vector, grouping on `model` and `owned`, and summarizing on `n()`. After piping the resulting dataframe into `ggplot()`, I used `geom_arg_bar()` faceted by `model` with colors manually assigned to `owned` using `scale_fill_manual()`. I also manually set panel spacing and text size using `theme()`. 

The results showed that for high-n models of moped, no models featured only owners or non-owners in reviews. In the analysis portion I use far fewer models than 20, but I wouldn't risk overfitting my model prior to receiving new data by selecting more within reason.

*characteristics of multiple variables*

Since the above analysis hadn't given me any clear information about correlation with our outcome variable or correlation between our independent variables, I then decided to run a principle component analysis and generate a scree and biplot to quickly get a better idea for what's going on in the data. When running the PCA I generated `commuter` as a binary of `used_for`, and dropped the remaining categorical variables that couldn't be easily one-hot encoded manually. I then scaled, omitted NA values, and ran `prcomp()`. 

To generate the biplot I used `tidy()` to pull the rotation matrix, transformed it using `pivot_wider()`, and fed the results into `ggplot()` using `geom_segment()` and custom aesthetic assignments stored in `arrow_style`. Limits were set to contain the resulting vectors without excessive white space. 

The resulting biplot showed clear correlation between `owned` and `commuter`, and a negative correlation between `owned`, `visual_appeal`, and `reliability`. The latter two also exhibited significant collinearity. The remaining variables were slightly negatively correlated with `owned` and collinear with one another. So moped owners commonly used the moped for commuting, reported that it was ugly as sin, and cited reliability as problematic. It would make sense that reviewers who only rent a moped wouldn't have time to be bothered by these issues - and it would make sense for fake reviews to give positive impressions of the models in question. 

To generate the scree plot I applied `tidy()` to the PCA object and piped it directly into `ggplot()` with the `geom_col()` aesthetic. To reach a suitable proportion of variance explained I'd need to include three or more principle components, so PCA is unsuitable for modeling in this case even though the biplot provided useful information about the data. 

Based on the results from the pie charts and summary tables, I knew that wrangling the `model` variable correctly was going to be an important factor in avoiding overfitting. Retaining all categories would lead to problematic overfitting, as the many small-n categories aren't representative samples from their respective population means. On the other hand, to maximize retained information for modeling I wanted to retain as much information as possible assigned to their respective observations. Theoretically, I was weighing two possibilities. First, morally onerous marketers from certain brands could be submitting fake reviews. This would lead to a high degree of correlation between the proportion of `owned == 1` values and brand name. Second, specific models could be more likely to be chosen as part of rental fleets - in which case the full model name would be the more appropriate unit for one-hot encoding. Retaining categories only at the brand level might also help retain information, since some brands may field many models such that their total market share comprises a substantial portion of the sample where individually they do not. 

*characteristics of a single variable*

To determine this, I generated two visualizations. First, a `geom_bar()` by the default model name sorted using `fct_infreq()` using a `geom_hline()` to show categories meeting the default sample proportion threshold from `dummyVars()` of 0.10. I used the color palette from `viridis` and `coord_flip()` for readability and ease of interpretation. Then I repeated the visualization, but I first used `separate()` to split the `model` variable so I could examine the `make` independently. 

The resulting plots showed that the same brands/models would be captured in either case, and that smaller brands didn't meet the threshold once their total market share was considered. 

*characteristics of multiple variables*

I then decided to compare the total number of observations captured in each treatment plan. To do this, I generated two dataframes named `moped_n_1` and `moped_n_2` containing summary tables of observation counts grouped by model/make respectively and filtered for categories containing the desired proportion. I then appended them using `rbind()` to form `model_n`, and performed additional transformations to ensure neat aesthetic assignments for ggplot. I piped the resulting dataframe into `ggplot()`, factoring on the `type` dummy variable and using `geom_col()` so I could assign the preexisting count information from the summary tables to the y-axis. 

The resulting plot showed that although the brands in both treatment plans were the same, one-hot encoding on `make` captured a substantially higher proportion of observations than encoding on `model`. 


```{r}
# problem type is binary classification

# seed for replication
set.seed(100)
  ###https://win-vector.com/2017/04/15/encoding-categorical-variables-one-hot-and-beyond/
  
# manually converting one variable to a dummy
moped <-
  moped_og |>
  mutate(commuter = ifelse(used_for == "Commuting", 1, 0), .keep = "unused")
  
# test/train split
split <-   
  initial_split(moped, prop = 0.8, strata = "model")

moped_train <- 
  training(split)

moped_test <- 
  testing(split)

# storing vtreat plan
treatplan <- designTreatmentsZ(moped_train, colnames(moped_train), minFraction = 1/10) 

# inspecting results
View(treatplan[["scoreFrame"]])

# executing treatment
train_treated <-  
  prepare(treatplan, moped_train) |>
  select(-model_catP)

test_treated <- 
  prepare(treatplan, moped_test)  |>
  select(-model_catP)

# log reg
  
  # model definition/training
  logreg_model_1 <- 
    glm(owned ~ ., data = train_treated, family = "binomial")
  
  logreg_model_1
  
  # summary of model
  summary(logreg_model_1)
  
  # test model
  test_treated$pred <- 
    predict(logreg_model_1, test_treated, type = "response")
  
  # saving this dataframe for later
  logreg_pred_1 <- 
    test_treated
    
# xgboost
  # defining dataframes sans outcome
    xgb_train <- 
      train_treated |>
      select(-owned) |>
      as.matrix()
      
    xgb_test <- 
      test_treated |>
      select(-c(pred, owned)) |>
      as.matrix()
      
  # running cross validation to find the ideal parameters
    cv <- xgb.cv(data = xgb_train, 
                 label = train_treated$owned,
                 nrounds = 100,
                 nfold = 5,
                 objective = "binary:logistic",
                 max_depth = 5,
                 early_stopping_rounds = 5, 
                 verbose = FALSE   # silent
      )
  # fetching evaluation log
    cv$evaluation_log |>
      summarize(ntrees.train = which.min(train_logloss_mean),
                ntrees.test = which.min(test_logloss_mean))
    
  # checking cross validation results using xgb.train()
  # generating appropriate matrices
  xgbDM_train <- 
    xgb.DMatrix(data = xgb_train, label = train_treated$owned)
    
  xgbDM_test <- 
    xgb.DMatrix(data = xgb_test, label = test_treated$owned)
  
  # generating watchlist
  watchlist <- 
    list(train = xgbDM_train, test = xgbDM_test)
  
  # running xgb.train() 
  xgb_training <- 
    xgb.train(
      data = xgbDM_train,
      max.depth = 5, 
      objective = "binary:logistic",
      watchlist = watchlist, 
      nrounds = 100,
      verbose = 0
    )
  
  # obtaining evaluation log
  xgb_training$evaluation_log |>
    summarize(ntrees.train = which.min(train_logloss),
              ntrees.test = which.min(test_logloss))
    
  # defining final model
  xgb_model_1 <- xgboost(data = xgb_train, 
                       label = train_treated$owned,
                       objective = "binary:logistic",
                       max.depth = 5, 
                       nrounds = 13, 
                       verbose = FALSE
                       )
  
  # predictions 
  test_treated$pred <- 
    predict(xgb_model_1, xgb_test, nrounds = 8)
  
  # saving for evaluation
  xgb_pred_1 <- 
    test_treated
  
  # saving colnames for evaluation
  colnames_1 <- 
    colnames(xgb_train)
```

```{r}
# models without stratification
  # test/train split
  split <-   
    initial_split(moped, prop = 0.8)
  
  moped_train <- 
    training(split)
  
  moped_test <- 
    testing(split)
  
  # storing new vtreat plan
  treatplan <- designTreatmentsZ(moped_train, colnames(moped_train), minFraction = 1/10) 
  
  # inspecting results
  View(treatplan[["scoreFrame"]])
  
  # executing treatment
  train_treated <-  
    prepare(treatplan, moped_train) |>
    select(-model_catP)
  
  test_treated <- 
    prepare(treatplan, moped_test)  |>
    select(-model_catP)
  
  # log reg
  # model definition/training
  logreg_model_2 <- 
    glm(owned ~ ., data = train_treated, family = "binomial")
  
  logreg_model_2
  
  # summary of model
  summary(logreg_model_2)
  
  # test model
  test_treated$pred <- 
    predict(logreg_model_2, test_treated, type = "response")
  
  # saving this dataframe for later
  logreg_pred_2 <- 
    test_treated
  
  # xgboost
  # defining dataframes sans outcome
  xgb_train <- 
    train_treated |>
    select(-owned) |>
    as.matrix()
  
  xgb_test <- 
    test_treated |>
    select(-c(pred, owned)) |>
    as.matrix()
  
  # generating appropriate matrices
  xgbDM_train <- 
    xgb.DMatrix(data = xgb_train, label = train_treated$owned)
  
  xgbDM_test <- 
    xgb.DMatrix(data = xgb_test, label = test_treated$owned)
  
  # running cross validation to find the ideal parameters
  cv <- xgb.cv(data = xgbDM_train,
               nrounds = 100,
               nfold = 5,
               objective = "binary:logistic",
               max_depth = 5,
               early_stopping_rounds = 5, 
               verbose = FALSE   # silent
  )
  # fetching evaluation log
  cv$evaluation_log |>
    summarize(ntrees.train = which.min(train_logloss_mean),
              ntrees.test = which.min(test_logloss_mean))
  
  # checking cross validation results using xgb.train()
  # generating watchlist
  watchlist <- 
    list(train = xgbDM_train, test = xgbDM_test)
  
  # running xgb.train() 
  xgb_training <- 
    xgb.train(
      data = xgbDM_train,
      max.depth = 5, 
      objective = "binary:logistic",
      watchlist = watchlist, 
      nrounds = 100,
      verbose = 0
    )
  
  # obtaining evaluation log
  xgb_training$evaluation_log |>
    summarize(ntrees.train = which.min(train_logloss),
              ntrees.test = which.min(test_logloss))
  
  # defining final model
  xgb_model_2 <- xgboost(data = xgbDM_train, 
                               objective = "binary:logistic",
                               max.depth = 5, 
                               nrounds = 14, 
                               verbose = FALSE
  )
  
  # predictions 
  test_treated$pred <- 
    predict(xgb_model_2, xgb_test, nrounds = 9)
  
  # saving for evaluation
  xgb_pred_2 <- 
    test_treated
  
  # saving colnames for evaluation
  colnames_2 <- 
    colnames(xgb_train)
```

```{r}
# models without `model`
  # test/train split
  split <-
    moped |>
    select(-model) |>
    initial_split(prop = 0.8)
  
  moped_train <- 
    training(split)
  
  moped_test <- 
    testing(split)
  
  # storing new vtreat plan
  treatplan <- designTreatmentsZ(moped_train, colnames(moped_train), minFraction = 1/10) 
  
  # inspecting results
  View(treatplan[["scoreFrame"]])
  
  # executing treatment
  train_treated <-  
    prepare(treatplan, moped_train)
  
  test_treated <- 
    prepare(treatplan, moped_test)
  
# log reg
  # model definition/training
  logreg_model_3 <- 
    glm(owned ~ ., data = train_treated, family = "binomial")
  
  logreg_model_3
  
  # summary of model
  summary(logreg_model_3)
  
  # test model
  test_treated$pred <- 
    predict(logreg_model_3, test_treated, type = "response")
  
  # saving this dataframe for evaluation
  logreg_pred_3 <- 
    test_treated
  
# xgboost
  # defining dataframes sans outcome
  xgb_train <- 
    train_treated |>
    select(-owned) |>
    as.matrix()
  
  xgb_test <- 
    test_treated |>
    select(-c(pred, owned)) |>
    as.matrix()
  
  # generating appropriate matrices
  xgbDM_train <- 
    xgb.DMatrix(data = xgb_train, label = train_treated$owned)
  
  xgbDM_test <- 
    xgb.DMatrix(data = xgb_test, label = test_treated$owned)
  
  
  # running cross validation to find the ideal parameters
  cv <- xgb.cv(data = xgbDM_train, 
               nrounds = 100,
               nfold = 5,
               objective = "binary:logistic",
               max_depth = 5,
               early_stopping_rounds = 5, 
               verbose = FALSE   # silent
  )
  # fetching evaluation log
  cv$evaluation_log |>
    summarize(ntrees.train = which.min(train_logloss_mean),
              ntrees.test = which.min(test_logloss_mean))
  
  # checking cross validation results using xgb.train()
  # generating watchlist
  watchlist <- 
    list(train = xgbDM_train, test = xgbDM_test)
  
  # running xgb.train() 
  xgb_training <- 
    xgb.train(
      data = xgbDM_train,
      max.depth = 5, 
      objective = "binary:logistic",
      watchlist = watchlist, 
      nrounds = 100,
      verbose = 0
    )
  
  # obtaining evaluation log
  xgb_training$evaluation_log |>
    summarize(ntrees.train = which.min(train_logloss),
              ntrees.test = which.min(test_logloss))
  
  # defining final model
  xgb_model_3 <- xgboost(data = xgbDM_train, 
                             objective = "binary:logistic",
                             max.depth = 5, 
                             nrounds = 15, 
                             verbose = FALSE
  )
  
  # predictions 
  test_treated$pred <- 
    predict(xgb_model_3, xgb_test, nrounds = 10)
  
  # saving for evaluation
  xgb_pred_3 <- 
    test_treated
  
  # saving variable names for evaluation
  colnames_3 <- 
    colnames(xgb_train)
```

## Model Fitting
Describe your approach to the model fitting. In your description you should:
- Describe what type of machine learning problem you are working on
- Describe which method you selected for the baseline model and explain why you chose this model
- Describe which method you selected for the comparison model and explain why you chose this model

Since `owned` is a binary classifier, it was clear that I was dealing with a binary classification problem. 

For the base model I selected logistic regression. Although I didn't feel it was likely to show exceptional performance based on the results from the data exploration and the simplicity of the methodology, it would be easy to examine and evaluate and its simplicity makes it useful as a baseline. Another advantage of this simplicity is that although logistic regression is unlikely to generate highly accurate predictions, the risk of overfitting or other serious problems is low.

For the comparison model I selected Xtreme Gradient Boosting. I chose `xgboost()` over random forest modeling for a number of reasons. Gradient boosting is better with unbalanced data, better at handling large numbers of categorical variables, and generally has a slight edge in performance. Random forest models are easier to tune, but my priority was model performance rather than speed. 

For the test/train split I used the standard 80/20 proportion, `initial_split()`, and the `vtreat` toolkit. The function `designTreatmentsZ()` has additional features for handling of `NA` values not available in `dummyVars()` from `caret`, so given the large number of `NA` values in the data I decided to use `vtreat`. I specified the `initial_split()` to stratify on `model` in order to avoid problems with non-representative sampling of models, and I removed `model_catP` after treatment to avoid metadata about `model` category proportion leading to overfitting. 

After splitting and preparing the data, I generated `logreg_model` by running `glm()` on the training data. I then used `predict()` to generate predicted values for the test dataframe and saved the results for model evaluation.

For the gradient boosting model I first used `select()` and `as.matrix()` to generate matrices of data appropriate for the `xgboost` toolkit, without their outcome variables.

I then ran a k-fold cross validation using `xgb.cv()` in conjunction with `xgb.train()` to find the highest performing parameters for the model. For `xgb.train()` I formatted the existing dataframes into `xgb.DMatrix` objects using the corresponding function and defined a `watchlist` of the test and train datasets. I then fed the resulting parameters into `xgboost()` using `objective = "binary:logistic"`, with a `max.depth` of 5. 

I experimented with parameter settings to ensure optimal performance, but was unable to improve AUC scores. The results are in the model evalution section. 

I then decided to experiment with a number of variations to better estimate performance in modeling future data. 

First, I reran the logistic regression and gradient boosting algorithms with unstratified samples. Depending on the use case for the model, this may be more descriptive of performance in future applications if the algorithm is being used to e.g. predict ownership on individual incoming reviews. The proportions of models prevalent in future reviews may not reflect the proportion of past data, and so the model may need to perform on data that does not fully resemble the training set. 

Second, I built models to perform estimations without using `model` to compare to the unstratified performance. Although the feature importance indicates that the stratified model places little relative importance on the results of categorical variables, the model is likely to precondition some trees on model membership and may improve in performance on unstratified samples without that information.

The code for these models is contained in the second and third code chunks above, and their performance metrics are contained in the model evaluation section below. 

```{r}
# Model Evaluation
# Choose a metric and evaluate the performance of the two models

# model set 1 evaluation
# logreg evaluation
  # glance to get model stats
  (perf <- glance(logreg_model_1))
  
  # calculating pseudo-R-squared
  (pseudoR2 <- 1 - perf$deviance/perf$null.deviance)
  
  # gain curve plot
  GainCurvePlot(logreg_pred_1, xvar = "pred", "owned", "Logistic regression model for moped ownership")
  
  # ROC curve
  ROCPlot(logreg_pred_1, 
          xvar = "pred", 
          truthVar = "owned", 
          truthTarget = TRUE,
          title = "Logistic regression model for moped ownership", 
          add_beta_ideal_curve = TRUE)
  
# xgb evaluation
  # gain curve plot
  GainCurvePlot(xgb_pred_1, xvar = "pred", "owned", "Xtreme Gradient Boosting model for moped ownership")
  
  # ROC curve #2
  ROCPlot(xgb_pred_1, 
          xvar = "pred", 
          truthVar = "owned", 
          truthTarget = TRUE,
          title = "Xtreme Gradient Boosting model for moped ownership", 
          add_beta_ideal_curve = TRUE)
  
  # inspecting feature importance
  (importance_matrix <- 
      xgb.importance(feature_names = colnames_1, 
                     model = xgb_model_1))
  
  # visualizing feature importance
  xgb.plot.importance(importance_matrix[1:14,])
```

```{r}
# model set 2 evaluation
  # logreg evaluation
  # glance to get model stats
  (perf <- glance(logreg_model_2))
  
  # calculating pseudo-R-squared
  (pseudoR2 <- 1 - perf$deviance/perf$null.deviance)
  
  # gain curve plot
  GainCurvePlot(logreg_pred_2, xvar = "pred", "owned", "Logistic regression model for moped ownership")
  
  # ROC curve
  ROCPlot(logreg_pred_2, 
          xvar = "pred", 
          truthVar = "owned", 
          truthTarget = TRUE,
          title = "Logistic regression model for moped ownership", 
          add_beta_ideal_curve = TRUE)
  
  # xgb evaluation
  # gain curve plot
  GainCurvePlot(xgb_pred_2, xvar = "pred", "owned", "Xtreme Gradient Boosting model for moped ownership")
  
  # ROC curve #2
  ROCPlot(xgb_pred_2, 
          xvar = "pred", 
          truthVar = "owned", 
          truthTarget = TRUE,
          title = "Xtreme Gradient Boosting model for moped ownership", 
          add_beta_ideal_curve = TRUE)
  
  # inspecting feature importance
  (importance_matrix <- 
      xgb.importance(feature_names = colnames_2, 
                     model = xgb_model_2))
  
  # visualizing feature importance
  xgb.plot.importance(importance_matrix[1:13,])
```

```{r}
# model set 3 evaluation
  # logreg evaluation
  # glance to get model stats
  (perf <- glance(logreg_model_3))
  
  # calculating pseudo-R-squared
  (pseudoR2 <- 1 - perf$deviance/perf$null.deviance)
  
  # gain curve plot
  GainCurvePlot(logreg_pred_3, xvar = "pred", "owned", "Logistic regression model for moped ownership")
  
  # ROC curve
  ROCPlot(logreg_pred_3, 
          xvar = "pred", 
          truthVar = "owned", 
          truthTarget = TRUE,
          title = "Logistic regression model for moped ownership", 
          add_beta_ideal_curve = TRUE)
  
  # xgb evaluation
  # gain curve plot
  GainCurvePlot(xgb_pred_3, xvar = "pred", "owned", "Xtreme Gradient Boosting model for moped ownership")
  
  # ROC curve #2
  ROCPlot(xgb_pred_3, 
          xvar = "pred", 
          truthVar = "owned", 
          truthTarget = TRUE,
          title = "Xtreme Gradient Boosting model for moped ownership", 
          add_beta_ideal_curve = TRUE)
  
  # inspecting feature importance
  (importance_matrix <- 
      xgb.importance(feature_names = colnames_3, 
                     model = xgb_model_3))
  
  # visualizing feature importance
  xgb.plot.importance(importance_matrix[1:11,])
```

## Model Evaluation
Explain what the results of your evaluation tell you. You should:
- Describe which metric you have selected to compare the models and why
- Explain what the outcome of this evaluation tells you about the performance of your models
- Identify, based on the evaluation, which you would consider to be the better performing approach

I decided on gain curve plots and ROC curves for comparison of model performance. I also examine pseudo-$R^2$ values and feature importance for the logistic regression and gradient boosting models respectively. However, gain curve plots and ROC plots are easily generated for both modeling types and thus best suited for comparison. 

In scenario 1, gradient boosting performs better in comparisons of both gain curves and ROC plots. The AUC of the gradient boosted model, 0.81, outperforms the regression model (AUC of 0.77) by a narrow margin of 0.04. This relative performance gain holds across the ROC plot - that is, the gradient boosted model performs better regardless of whether specificity or sensitivity is desired. The regression model does have a slight edge in Gini coefficients, but otherwise gradient boosting is a strictly better choice. 

Model sets 2 and 3 are less clear. Logistic regression outperforms gradient boosting in terms of AUC in both cases, with scores of 0.77 vs. 0.72 and 0.75 vs. 0.72 respectively. This edge in performance comes primarily from high-specificity optimizations where logistic regression displays a notable edge. For problems where a high degree of sensitivity is desirable, gradient boosting can outperform regression. In model set 3 this is particularly apparent - but when compared to the performance of regression in model set 2 for sensitivity, gradient boosting is at best equal. Both sets of models do show a lower Gini coefficient for gradient boosting, but when the population distribution of models is unknown 
